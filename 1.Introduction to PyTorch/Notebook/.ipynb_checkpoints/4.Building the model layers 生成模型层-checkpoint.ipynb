{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30782fd1-8899-44a2-b032-3e797e8a680c",
   "metadata": {},
   "source": [
    "# Building the model layers 生成模型层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e100f9-1145-45f5-a7a7-5a8cc09cea41",
   "metadata": {},
   "source": [
    "# What is a neural network 什么是神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906286a-374e-4067-9130-cd22aab95af4",
   "metadata": {},
   "source": [
    "神经网络是按层连接的**神经元**的集合。每个神经元都是一个小的计算单元，执行简单的计算来共同解决问题。神经元分为 3 种类型的层：输入层、隐藏层和输出层。隐藏层和输出层包含许多神经元。神经网络模仿人脑处理信息的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02681e3c-d4d5-4abe-8d99-e9dbbeb5568a",
   "metadata": {},
   "source": [
    "# Components of a neural network 神经网络的组成部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ed341-286f-4bc7-9f67-cb4400776622",
   "metadata": {},
   "source": [
    "- **activation function** **激活函数** 决定神经元是否应该被激活。神经网络中发生的计算包括应用激活函数。如果神经元激活，则意味着输入很重要。有不同种类的激活函数。选择使用哪个激活函数取决于您想要的输出。激活函数的另一个重要作用是为模型添加非线性。\r\n",
    "  - *Binary* 如果函数结果为正，则用于将输出节点设置为 1；如果函数结果为零或负，则将输出节点设置为 0。$f(x)= {\\small \\begin{cases} 0, & \\text{if } x < 0\\\\ 1, & \\text{if } x\\geq 0\\\\ \\end{cases}}$\r\n",
    "  - *Sigmoid* 用于预测输出节点介于 0 和 1 之间的概率。$f(x) = {\\large \\frac{1}{1+e^{-x}}} $\r\n",
    "  - *Tanh* 用于预测输出节点是否在 1 到 -1 之间，用于分类用例。$f(x) = {\\large \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}} $\r\n",
    "  - *ReLU* (*rectified linear activation function*) 如果函数结果为负，则用于将输出节点设置为 0；如果结果为正，则保持结果值。$f(x)= {\\small \\begin{cases} 0, & \\text{if } x < 0\\\\ x, & \\text{if } x\\geq 0\\\\ \\end{cases}}$\r\n",
    "\r\n",
    "- **Weights** **权重** 影响我们网络的输出与预期输出值的接近程度。当输入进入神经元时，它会乘以权重值，所得输出要么被观察，要么被传递到神经网络中的下一层。一层中所有神经元的权重被组织成一个张量。\r\n",
    "\r\n",
    "- **Bias** **偏差** 弥补了激活函数的输出与其预期输出之间的差异。低偏差值表明网络对输出形式做出更多假设，而高偏差值对输出形式做出更少假设。\r\n",
    "\r\n",
    "我们可以说，具有weights $W$ 和bias $b$ 的神经网络层的输出 $y$ 的计算为，输入乘以 weights加上bias的总和。 $x = \\sum{(weights * inputs) + bias} $，其中 $f(x)$ 是激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac47f2-2a89-4d67-85ef-475169ef8578",
   "metadata": {},
   "source": [
    "# Build a neural network 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52a1ef-d013-44c0-afc6-e4d57ec8e18e",
   "metadata": {},
   "source": [
    "神经网络由对数据执行操作的层/模块组成。[torch.nn](https://pytorch.org/docs/stable/nn.html)命名空间提供了构建您自己的神经网络所需的所有构建块。在PyTorch 中，每个模块都是[nn.Module 的](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)子类。神经网络本身就是一个模块，由其他模块（层）组成。这种嵌套结构允许轻松构建和管理复杂的架构。\r\n",
    "\r\n",
    "在以下部分中，我们将构建一个神经网络，来对 FashionMNIST 数据集中的图像进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a5a878-35db-4f4c-99ec-e0b0d5c8c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609223c3-9c13-447f-a7a9-93ffee47eaaf",
   "metadata": {},
   "source": [
    "# Get Device for training 获取训练设备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420d4ac-825b-43c9-b998-400149ddf0b1",
   "metadata": {},
   "source": [
    "我们希望能够在 GPU 等硬件加速器（如果可用）上训练我们的模型。让我们检查一下[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) 或[torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html)是否可用，否则我们使用 CPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ef0a7c-3ecd-4dc3-859d-119f81f73eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df18a-882f-4491-a405-19162922582d",
   "metadata": {},
   "source": [
    "# Define the Class 定义类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3f963-6ca4-4bd9-ba91-07a34f9e98aa",
   "metadata": {},
   "source": [
    "我们通过子类化`nn.Module`来定义我们的神经网络。在`__init__`中，初始化神经网络层。每个`nn.Module`子类都在`forward`方法中实现对输入数据的操作。\n",
    "\n",
    "我们的神经网络由以下部分组成：\r\n",
    "\r\n",
    "- 输入层具有 28x28 或 784 个特征/像素。\r\n",
    "- 第一个线性模块采用输入 784 个特征，并将其转换为具有 512 个特征的隐藏层。 \r\n",
    "- ReLU 激活函数将应用于转换中。\r\n",
    "- 第二个线性模块将第一个隐藏层的 512 个特征作为输入，并将其转换到具有 512 个特征的下一个隐藏层。 \r\n",
    "- ReLU 激活函数将应用于转换中。\r\n",
    "- 第三个线性模块将 512 个特征作为来自第二个隐藏层的输入，并将这些特征转换到输出层，其中 10 是类的数量。 \r\n",
    "- ReLU 激活函数将应用于转换中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbd7ff5-32e9-4c07-b198-ea6eec06c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a3e8f-d2f6-480f-8aba-86377c4ddade",
   "metadata": {},
   "source": [
    "我们创建`NeuralNetwork` 的一个实例，并将其移动到`device`，并打印其结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a8a648-c206-464f-afca-f3c95ac3f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872c0c5-0f5c-44be-a512-4101eace7d94",
   "metadata": {},
   "source": [
    "为了使用该model，我们将输入数据传递给它。这将执行model的`forward`以及一些[background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866)。不要直接调用`model.forward()`！在输入上调用model会返回一个二维张量，其中 dim=0 对应于每个类的 10 个原始predicted values的每个输出，dim=1 对应于每个输出的各个值。\r\n",
    "\r\n",
    "我们通过，将它传递给`nn.Softmax`模块的实例，来获得prediction probabilities。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b27c24-3d44-4f40-bf5b-396dcfb7f014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device = device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim = 1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e93464-0a98-433a-adb2-6161d7da91ec",
   "metadata": {},
   "source": [
    "#  Weight and Bias 权重和偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e140ea2-f19c-4049-b356-a925d3f773cb",
   "metadata": {},
   "source": [
    "`nn.Linear` 模块随机初始化每层的权重和偏差，并在内部将值存储在张量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8fbdd53-01d0-4da4-8c1c-9026fbb11751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Linear Weights: Parameter containing:\n",
      "tensor([[ 1.8239e-02,  3.2942e-02, -3.2946e-02,  ...,  1.0153e-02,\n",
      "         -8.4223e-03,  4.5333e-05],\n",
      "        [ 2.9481e-02,  2.3620e-02,  1.5626e-02,  ...,  2.4684e-02,\n",
      "         -1.7073e-02,  2.8341e-02],\n",
      "        [ 3.4109e-02,  2.7542e-02,  2.9627e-02,  ...,  2.7297e-02,\n",
      "         -6.7970e-03,  2.1646e-02],\n",
      "        ...,\n",
      "        [ 5.7005e-03, -9.1041e-03, -3.0561e-02,  ..., -2.5262e-02,\n",
      "         -3.0317e-02,  3.4043e-02],\n",
      "        [-3.3178e-02, -8.2961e-03, -3.1984e-02,  ..., -3.2255e-02,\n",
      "         -3.2282e-02, -3.1796e-02],\n",
      "        [ 1.5601e-02,  8.5908e-03,  3.3128e-02,  ..., -1.3665e-02,\n",
      "         -1.2585e-02,  3.0685e-02]], device='cuda:0', requires_grad=True) \n",
      "\n",
      "First Linear biases: Parameter containing:\n",
      "tensor([ 0.0050,  0.0261, -0.0138, -0.0268,  0.0242,  0.0189,  0.0289, -0.0070,\n",
      "        -0.0327,  0.0235, -0.0189, -0.0075, -0.0213,  0.0308,  0.0036,  0.0250,\n",
      "        -0.0285, -0.0124, -0.0291,  0.0307,  0.0248,  0.0338, -0.0125,  0.0318,\n",
      "         0.0317,  0.0052,  0.0184,  0.0065, -0.0031, -0.0198,  0.0223,  0.0177,\n",
      "         0.0183, -0.0259,  0.0037,  0.0089,  0.0192, -0.0232, -0.0192, -0.0084,\n",
      "         0.0304,  0.0162, -0.0051, -0.0282,  0.0087,  0.0072,  0.0100,  0.0308,\n",
      "        -0.0023, -0.0167, -0.0206, -0.0003,  0.0155,  0.0094, -0.0193, -0.0136,\n",
      "         0.0283,  0.0032, -0.0018, -0.0166, -0.0052,  0.0037,  0.0066,  0.0041,\n",
      "         0.0261,  0.0023,  0.0298,  0.0034,  0.0120,  0.0262, -0.0209, -0.0082,\n",
      "         0.0183, -0.0312, -0.0154, -0.0253,  0.0019, -0.0237,  0.0265, -0.0257,\n",
      "        -0.0308,  0.0270, -0.0004,  0.0240, -0.0317, -0.0171, -0.0314, -0.0299,\n",
      "         0.0208, -0.0214, -0.0144,  0.0216,  0.0033, -0.0017,  0.0057, -0.0345,\n",
      "        -0.0348, -0.0113, -0.0284,  0.0143,  0.0261, -0.0139,  0.0226,  0.0242,\n",
      "        -0.0305,  0.0207,  0.0033,  0.0085,  0.0157, -0.0185,  0.0041,  0.0148,\n",
      "         0.0336, -0.0035, -0.0241,  0.0003, -0.0131, -0.0232, -0.0066, -0.0331,\n",
      "        -0.0268, -0.0231, -0.0294, -0.0156,  0.0255,  0.0242, -0.0164,  0.0333,\n",
      "        -0.0101,  0.0055,  0.0346, -0.0277,  0.0242,  0.0189,  0.0167,  0.0020,\n",
      "        -0.0055, -0.0256,  0.0035, -0.0042, -0.0344, -0.0233, -0.0231,  0.0066,\n",
      "        -0.0278,  0.0286,  0.0295, -0.0004, -0.0209, -0.0316,  0.0332,  0.0178,\n",
      "         0.0235, -0.0024,  0.0291,  0.0158, -0.0153, -0.0226, -0.0061, -0.0318,\n",
      "        -0.0197,  0.0079, -0.0071, -0.0246,  0.0187, -0.0208, -0.0237,  0.0154,\n",
      "         0.0195, -0.0102, -0.0243, -0.0188,  0.0093, -0.0300, -0.0246,  0.0042,\n",
      "         0.0298,  0.0082,  0.0245,  0.0089, -0.0166,  0.0246, -0.0163,  0.0169,\n",
      "        -0.0317,  0.0180, -0.0023, -0.0223,  0.0211, -0.0150, -0.0081,  0.0195,\n",
      "         0.0279, -0.0298,  0.0336,  0.0345, -0.0117,  0.0225, -0.0192,  0.0310,\n",
      "        -0.0346, -0.0142, -0.0194, -0.0316, -0.0105,  0.0302, -0.0251, -0.0145,\n",
      "         0.0018,  0.0345, -0.0152,  0.0277,  0.0330, -0.0241, -0.0153, -0.0254,\n",
      "         0.0108, -0.0131, -0.0031, -0.0262, -0.0192, -0.0258,  0.0241,  0.0219,\n",
      "        -0.0042,  0.0348, -0.0120, -0.0246, -0.0315,  0.0043, -0.0040, -0.0213,\n",
      "        -0.0105, -0.0242, -0.0086,  0.0093, -0.0169, -0.0062, -0.0319,  0.0209,\n",
      "         0.0147, -0.0033,  0.0338,  0.0066, -0.0086, -0.0021, -0.0165,  0.0059,\n",
      "        -0.0018, -0.0025, -0.0240,  0.0158,  0.0022,  0.0079, -0.0131,  0.0128,\n",
      "        -0.0349,  0.0246, -0.0105, -0.0180, -0.0096, -0.0064, -0.0293,  0.0275,\n",
      "        -0.0282, -0.0232, -0.0315,  0.0164, -0.0177, -0.0341,  0.0192, -0.0034,\n",
      "        -0.0161, -0.0135, -0.0344,  0.0283, -0.0190,  0.0183, -0.0085,  0.0235,\n",
      "         0.0352,  0.0137, -0.0010, -0.0026,  0.0126, -0.0137, -0.0094,  0.0125,\n",
      "         0.0214,  0.0139, -0.0127,  0.0236,  0.0255,  0.0039, -0.0012,  0.0307,\n",
      "        -0.0316,  0.0171,  0.0082, -0.0155,  0.0051,  0.0145,  0.0309,  0.0220,\n",
      "        -0.0214, -0.0077, -0.0174, -0.0062,  0.0241,  0.0237,  0.0043, -0.0276,\n",
      "         0.0041, -0.0111,  0.0211, -0.0262,  0.0123, -0.0055, -0.0292,  0.0241,\n",
      "        -0.0207, -0.0056, -0.0117,  0.0194,  0.0117, -0.0017,  0.0325,  0.0039,\n",
      "        -0.0037,  0.0338, -0.0323,  0.0138,  0.0067,  0.0193,  0.0181, -0.0235,\n",
      "        -0.0080,  0.0262, -0.0306, -0.0144,  0.0245,  0.0097, -0.0311,  0.0208,\n",
      "         0.0216, -0.0173,  0.0209,  0.0110,  0.0319, -0.0045, -0.0164,  0.0264,\n",
      "         0.0129, -0.0176,  0.0235, -0.0164, -0.0165, -0.0204,  0.0078,  0.0174,\n",
      "        -0.0272, -0.0050,  0.0015,  0.0177,  0.0273, -0.0286,  0.0193, -0.0131,\n",
      "         0.0066,  0.0351, -0.0327, -0.0247,  0.0019, -0.0043,  0.0322,  0.0208,\n",
      "        -0.0281,  0.0080,  0.0315, -0.0171, -0.0007, -0.0221, -0.0062,  0.0178,\n",
      "         0.0153,  0.0092,  0.0212,  0.0030,  0.0317,  0.0113,  0.0343,  0.0327,\n",
      "         0.0073,  0.0216, -0.0008, -0.0168, -0.0124,  0.0214,  0.0043,  0.0080,\n",
      "         0.0340, -0.0290,  0.0092, -0.0049, -0.0215, -0.0134, -0.0130, -0.0065,\n",
      "         0.0148,  0.0252, -0.0267, -0.0336,  0.0047, -0.0106, -0.0293, -0.0071,\n",
      "         0.0149,  0.0248,  0.0276,  0.0266, -0.0045,  0.0178,  0.0327,  0.0176,\n",
      "         0.0088, -0.0252,  0.0195,  0.0336,  0.0216, -0.0206,  0.0345,  0.0259,\n",
      "        -0.0347, -0.0102, -0.0059,  0.0033, -0.0030,  0.0015,  0.0203, -0.0086,\n",
      "        -0.0105, -0.0121, -0.0122,  0.0018, -0.0075, -0.0114,  0.0328, -0.0108,\n",
      "         0.0117,  0.0116, -0.0025, -0.0259, -0.0196, -0.0260,  0.0283, -0.0252,\n",
      "         0.0205, -0.0101, -0.0083, -0.0314,  0.0257,  0.0062,  0.0080, -0.0257,\n",
      "        -0.0085,  0.0086,  0.0346,  0.0073, -0.0145, -0.0109, -0.0095,  0.0044,\n",
      "         0.0086,  0.0166, -0.0238,  0.0068,  0.0177, -0.0257, -0.0104,  0.0311,\n",
      "         0.0088, -0.0196, -0.0298, -0.0241, -0.0157, -0.0205,  0.0064, -0.0127,\n",
      "         0.0264,  0.0266,  0.0343, -0.0022, -0.0134,  0.0353, -0.0166, -0.0286,\n",
      "         0.0085,  0.0218,  0.0094,  0.0193, -0.0029,  0.0070, -0.0109, -0.0341,\n",
      "         0.0102,  0.0069,  0.0102,  0.0066,  0.0005, -0.0357,  0.0329, -0.0297],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"First Linear Weights: {model.linear_relu_stack[0].weight} \\n\")\n",
    "print(f\"First Linear biases: {model.linear_relu_stack[0].bias} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b40ba3-be1d-4241-88d1-73b69df126f3",
   "metadata": {},
   "source": [
    "# Model Layers 模型层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9e402-8912-4c47-b833-cb6806fb52c1",
   "metadata": {},
   "source": [
    "让我们分解 FashionMNIST model中的layers。为了说明这一点，我们将采用 3 张大小为 28x28 的图像的小批量样本，看看当我们将其传递到网络时会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60a9ff4-5808-4693-afce-85127a0b2725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b21298-7c4b-4809-897d-73c46ee1891e",
   "metadata": {},
   "source": [
    "## nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9704f-1bdd-4fb2-9c70-1b062a23965a",
   "metadata": {},
   "source": [
    "我们初始化[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)  layer，将每个 2D 28x28 图像转换为 784 个像素值的连续数组，维持小批量维度（在 dim=0 时）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ab8dae-6d87-4648-bfbe-540f08232381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ce096-f8d0-486c-a8cb-f819c221de40",
   "metadata": {},
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8d5d1-2471-44df-b0be-7a4f2f406e09",
   "metadata": {},
   "source": [
    " [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)是一个使用其存储的权重和偏差对输入应用线性变换的模块。输入层中每个像素的灰度值将连接到隐藏层中的神经元进行计算。用于转换的计算是 ${{weight * input + bias}} $。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b6b3ce-4eea-40df-9571-58d1eb8cb9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features = 28 * 28, out_features = 20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ea15-a945-40ec-88ad-8633247cacfb",
   "metadata": {},
   "source": [
    "## nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba876d1c-3e05-455e-97e4-c75f9b1f881a",
   "metadata": {},
   "source": [
    "非线性激活是在模型的输入和输出之间创建复杂映射的原因。它们在线性变换后应用以引入*非线性*，帮助神经网络学习各种现象。\r\n",
    "\r\n",
    "在此模型中，我们在线性层之间使用[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)，但还有其他激活可以在模型中引入非线\n",
    "\n",
    "ReLU 激活函数获取线性层计算的输出，并将负值替换为零。\n",
    "\n",
    "Linear output: ${ x = {weight * input + bias}} $。\n",
    "\n",
    "ReLU: \n",
    "$\n",
    "f(x)= \n",
    "\\begin{cases}\n",
    "  0, & \\text{if } x < 0\\\\\n",
    "  x, & \\text{if } x\\geq 0\\\\\n",
    "\\end{cases}\n",
    "$性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c4f5f1f-e9eb-4b86-943b-e2fa6aaa2e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.0766, -0.0625, -0.2205, -0.0216, -0.2053, -0.2668, -0.0165, -0.2928,\n",
      "          0.1630,  0.3305,  0.0926,  0.1549, -0.6023,  0.5706,  0.1748,  0.5451,\n",
      "          0.3832, -0.2599,  0.4527,  0.3424],\n",
      "        [ 0.4078, -0.3803,  0.1257, -0.4406, -0.2253, -0.1402, -0.2847, -0.3468,\n",
      "         -0.2220,  0.5142,  0.1939, -0.0116, -0.5433,  0.7497,  0.0924, -0.0852,\n",
      "          0.2829, -0.4219,  0.0996, -0.0094],\n",
      "        [ 0.4012, -0.2364, -0.2639, -0.4588, -0.0886, -0.6564, -0.3471, -0.4169,\n",
      "          0.1507,  0.6246,  0.3622, -0.1566, -0.3550,  0.4770, -0.0493, -0.1288,\n",
      "          0.3137,  0.1556,  0.0782, -0.1290]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0766, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1630,\n",
      "         0.3305, 0.0926, 0.1549, 0.0000, 0.5706, 0.1748, 0.5451, 0.3832, 0.0000,\n",
      "         0.4527, 0.3424],\n",
      "        [0.4078, 0.0000, 0.1257, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5142, 0.1939, 0.0000, 0.0000, 0.7497, 0.0924, 0.0000, 0.2829, 0.0000,\n",
      "         0.0996, 0.0000],\n",
      "        [0.4012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1507,\n",
      "         0.6246, 0.3622, 0.0000, 0.0000, 0.4770, 0.0000, 0.0000, 0.3137, 0.1556,\n",
      "         0.0782, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b4310-ebb5-4477-ae69-0e1657b67c50",
   "metadata": {},
   "source": [
    "## nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e382125-edae-44f9-877d-57b5d9e660a5",
   "metadata": {},
   "source": [
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)是模块的有序容器。数据按照定义的相同顺序传递通过所有模块。您可以使用顺序容器来组合一个快速网络，例如`seq_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8fe7e2-7fd9-4bf5-8c25-1d579983115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba583f-fdb6-4d2a-ab54-1d5b25109f41",
   "metadata": {},
   "source": [
    "## nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d952ed-55f2-4af2-9873-500af32699f6",
   "metadata": {},
   "source": [
    "神经网络的最后一个线性层返回logits （ [-infty, infty] 中的原始值）被传递到 [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)模块。Softmax激活函数用于计算神经网络输出的概率。它仅用于神经网络的输出层。Logits 缩放为值 [0, 1]，表示模型对每个类别的预测概率。`dim`参数指示维度，沿该维度值的总和必须为 1。具有最高概率的节点预测所需的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29fdef10-8829-49ea-a544-17ab0a571c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b41da3-f55a-45ba-b9cd-b78492972df5",
   "metadata": {},
   "source": [
    "# Model Parameters 模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ab4b2-64dc-4e05-b572-4f9c9d64e653",
   "metadata": {},
   "source": [
    "神经网络内的许多层都是*参数化的*。在训练期间，优化的相关权重和偏差。子类化`nn.Module`会自动跟踪模型对象中定义的所有字段，并使所有参数都可以使用模型`parameters()`或`named_parameters()`方法进行访问。\r\n",
    "\r\n",
    "在此示例中，我们迭代每个参数，并打印其大小及其值的预览。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "213aaf09-36fa-49e4-b9b9-0bd6f91650b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[-0.0337,  0.0320,  0.0072,  ..., -0.0061, -0.0181,  0.0175],\n",
      "        [ 0.0285,  0.0009,  0.0083,  ...,  0.0137, -0.0123,  0.0303]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([-0.0018, -0.0297], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[-0.0172,  0.0179,  0.0206,  ..., -0.0175, -0.0417,  0.0410],\n",
      "        [ 0.0051, -0.0359,  0.0321,  ...,  0.0005,  0.0167,  0.0055]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([-0.0300, -0.0200], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[ 0.0104,  0.0304,  0.0023,  ...,  0.0159, -0.0316,  0.0228],\n",
      "        [-0.0337, -0.0182,  0.0037,  ...,  0.0105, -0.0120,  0.0311]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([ 0.0134, -0.0431], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570081f3-7b61-4525-a8ce-6195b6acfa9c",
   "metadata": {},
   "source": [
    "# 知识检查"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f854f-b14b-41a7-b26a-9e60c1ebdd1a",
   "metadata": {},
   "source": [
    "PyTorch 中所有神经网络模块的基类为 `torch.nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd2aec-7bc2-41d9-a543-4084c3aa6546",
   "metadata": {},
   "source": [
    "# Further Reading 进一步阅读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7edbd-bff4-4d56-b988-dfeebd1a4ec1",
   "metadata": {},
   "source": [
    "- [torch.nn API](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589202a-0a93-4a39-a091-38c6afb4808a",
   "metadata": {},
   "source": [
    "# References 参考资料"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1403b0-3827-41b8-9f4a-fb48e7028d4e",
   "metadata": {},
   "source": [
    "使用 PyTorch 进行机器学习的简介 - Training | Microsoft Learn\n",
    "\n",
    "[使用 PyTorch 进行机器学习的简介 - Training | Microsoft Learn](https://learn.microsoft.com/zh-cn/training/modules/intro-machine-learning-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17499ead-4ba0-45c0-908c-59ead9e89217",
   "metadata": {},
   "source": [
    "# Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8999e5ea-929a-43dd-9fb1-f885d904b230",
   "metadata": {},
   "source": [
    "storm-ice/PyTorch_Fundamentals\r\n",
    "\r\n",
    "[storm-ice/PyTorch_Fundamentals](https://github.com/storm-ice/PyTorch_Fundamentals/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e173b-558c-4a7d-8e11-edbf7367e279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
