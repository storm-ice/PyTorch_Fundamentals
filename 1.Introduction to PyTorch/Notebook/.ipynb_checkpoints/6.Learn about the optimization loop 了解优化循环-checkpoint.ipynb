{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29be191-d3ac-4a75-95bb-cc2f981da1b6",
   "metadata": {},
   "source": [
    "# Learn about the optimization loop 了解优化循环"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65a7d1-40f6-4345-aab3-49dca4e0e0ec",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters 优化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d2f22-8a7b-44c6-8e73-651ab4451ea3",
   "metadata": {},
   "source": [
    "现在我们有了模型和数据，是时候通过优化数据上的参数来训练、验证和测试我们的模型了。训练模型是一个迭代过程；在每次迭代中，模型都会对输出进行猜测，计算其猜测中的误差（*损失*），收集相对于其参数的导数的误差（如我们在[上一节](https://pytorch.org/tutorials/beginner/basics/autograd_tutorial.html)中看到的），并使用梯度下降**优化**这些参数。有关此过程的更详细演练，请观看[3Blue1Brown 的反向传播](https://www.youtube.com/watch?v=tIeHLnjs5U8)有关视频。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c123f9f-c771-4c69-9ab0-84669084a147",
   "metadata": {},
   "source": [
    "## Prerequisite Code 前置代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ca765-defa-42e1-b1cf-9d7ecabe0c69",
   "metadata": {},
   "source": [
    "我们加载前面有关[数据集和数据加载器](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) 以及[构建模型](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a8152c-9cb6-4af1-b959-c507f2f62bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45222a02-5a4c-485f-b62a-f27b2d2893bf",
   "metadata": {},
   "source": [
    "## Setting hyperparameters 设置超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7bc9a-ebe8-4c27-9282-afb2147dbcd4",
   "metadata": {},
   "source": [
    "超参数是可调整的参数，可让您控制模型优化过程。不同的超参数值会影响模型训练和收敛速度（[阅读](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)有关超参数调整的更多信息）\r\n",
    "\r\n",
    "我们定义以下训练超参数：\r\n",
    "\r\n",
    "-  **Number of Epochs** - 整个训练数据集通过网络的次数。\r\n",
    "-  **Batch Size** - 每个Epoch模型看到的数据样本数量。迭代完成一个epoch所需的批次数。\r\n",
    "-  **Learning Rate**- 模型在搜索可产生更高模型精度的最佳权重时匹配的步长大小。值越小意味着模型需要更长的时间才能找到最佳权重。较大的值可能会导致模型跳过并错过最佳权重，从而在训练期间产生不可预测的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46340230-38e3-4b18-8827-dd7b7e8539bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f302e-da31-4775-940a-029848f60b26",
   "metadata": {},
   "source": [
    "## Add an optimization loop 添加优化循环"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df041b36-2b41-4e34-975b-4b8bb0e629e7",
   "metadata": {},
   "source": [
    "一旦我们设置了超参数，我们就可以使用优化循环来训练和优化我们的模型。优化循环的每次迭代称为一个**epoch**。\n",
    "\n",
    "每个 epoch由两个主要部分组成：\n",
    "\n",
    "-  **The Train Loop** - 迭代训练数据集并尝试收敛到最佳参数。\n",
    "-  **The Validation/Test Loop** - 迭代测试数据集以检查模型性能是否有所改善。\n",
    "\n",
    "让我们简单熟悉一下训练循环中使用的一些概念。向前跳转查看优化循环的[完整实现](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-impl-label)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd39d24-73af-449b-a55c-d3836c949838",
   "metadata": {},
   "source": [
    "### Add a loss function 添加损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa301a0c-1a78-41d5-b806-b90760042318",
   "metadata": {},
   "source": [
    "当提供一些训练数据时，我们未经训练的网络可能不会给出正确的答案。**损失函数**衡量的是得到的结果与目标值的不相似程度，它是我们在训练时想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。\n",
    "\n",
    "常见的损失函数包括：\n",
    "- [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)（Mean Square Error 均方误差） 用于回归任务\n",
    "- [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)（Negative Log Likelihood 负对数似然） 用于分类\n",
    "- [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) 结合了`nn.LogSoftmax`和`nn.NLLLoss`。\n",
    "\n",
    "我们将模型的输出 logits 传递给`nn.CrossEntropyLoss`，这将标准化 logits 并计算预测误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e13731-5333-4c07-8bd9-a6847a6558f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb188e4-27cf-4aa3-a739-461bb0be5d27",
   "metadata": {},
   "source": [
    "###  Optimization pass 优化传递"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e02b75-5a2f-4d22-84ad-4f1ed43534c6",
   "metadata": {},
   "source": [
    "优化是调整模型参数以减少每个训练步骤中模型误差的过程。**Optimization algorithms**定义了如何执行此过程（在本例中我们使用 *Stochastic Gradient Descent* *随机梯度下降*）。所有优化逻辑都封装在`optimizer`对象中。这里，我们使用SGD优化器；此外，PyTorch 中还有许多[不同的优化器](https://pytorch.org/docs/stable/optim.html) ，例如 ADAM 和 RMSProp，它们可以更好地处理不同类型的模型和数据。\r\n",
    "\r\n",
    "注册需要训练的模型参数，并传入学习率超参数。我们通过这种方式，来初始化优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb578000-eac6-437a-b63b-e5f43ccda4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9696bc-d18b-4db7-bb91-0c84e5e3d304",
   "metadata": {},
   "source": [
    "在训练循环中，优化分三个步骤进行：\r\n",
    "\r\n",
    "- 调用`optimizer.zero_grad()`重置模型参数的梯度。默认情况下渐变相加；为了防止重复计算，我们在每次迭代时明确地将它们归零。\r\n",
    "- 通过调用`loss.backward()`来反向传播预测损失。PyTorch 存储每个参数的损失梯度。\r\n",
    "- 一旦我们有了梯度，通过后向传递中收集的梯度，我们就可以调用`optimizer.step()`来调整参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c87f9-5b5d-47e7-b7fb-9893bbc7b1db",
   "metadata": {},
   "source": [
    "## Full Implementation 全面实施"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76d7f6-cb14-42b6-961b-92d608bf595a",
   "metadata": {},
   "source": [
    "我们定义了`train_loop`优化代码的循环，`test_loop`根据我们的测试数据评估模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e668f182-c043-46ca-8082-d66cc5e7ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} {current:>5d}/{size:>5d}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd07bf0-b066-49a7-93f5-b38131b3aa9a",
   "metadata": {},
   "source": [
    "我们初始化损失函数和优化器，并将其传递给`train_loop`和`test_loop`。请随意增加epoch数来跟踪模型改进的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569db738-a004-4f89-8122-0d6e14a884bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      " ----------\n",
      "loss: 2.301911     0/60000\n",
      "loss: 2.292816  6400/60000\n",
      "loss: 2.287881 12800/60000\n",
      "loss: 2.287051 19200/60000\n",
      "loss: 2.255377 25600/60000\n",
      "loss: 2.253076 32000/60000\n",
      "loss: 2.260443 38400/60000\n",
      "loss: 2.247314 44800/60000\n",
      "loss: 2.241305 51200/60000\n",
      "loss: 2.210870 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 2.231190 \n",
      "\n",
      "Epoch 2 \n",
      " ----------\n",
      "loss: 2.239081     0/60000\n",
      "loss: 2.230416  6400/60000\n",
      "loss: 2.228071 12800/60000\n",
      "loss: 2.236214 19200/60000\n",
      "loss: 2.153478 25600/60000\n",
      "loss: 2.158298 32000/60000\n",
      "loss: 2.178127 38400/60000\n",
      "loss: 2.160697 44800/60000\n",
      "loss: 2.154827 51200/60000\n",
      "loss: 2.075521 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.133443 \n",
      "\n",
      "Epoch 3 \n",
      " ----------\n",
      "loss: 2.146330     0/60000\n",
      "loss: 2.128829  6400/60000\n",
      "loss: 2.128925 12800/60000\n",
      "loss: 2.157317 19200/60000\n",
      "loss: 1.988493 25600/60000\n",
      "loss: 2.024168 32000/60000\n",
      "loss: 2.047211 38400/60000\n",
      "loss: 2.034990 44800/60000\n",
      "loss: 2.041876 51200/60000\n",
      "loss: 1.895464 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.005870 \n",
      "\n",
      "Epoch 4 \n",
      " ----------\n",
      "loss: 2.021032     0/60000\n",
      "loss: 1.997008  6400/60000\n",
      "loss: 2.011971 12800/60000\n",
      "loss: 2.068262 19200/60000\n",
      "loss: 1.803704 25600/60000\n",
      "loss: 1.892051 32000/60000\n",
      "loss: 1.908679 38400/60000\n",
      "loss: 1.918824 44800/60000\n",
      "loss: 1.930000 51200/60000\n",
      "loss: 1.739982 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 1.891897 \n",
      "\n",
      "Epoch 5 \n",
      " ----------\n",
      "loss: 1.902151     0/60000\n",
      "loss: 1.882296  6400/60000\n",
      "loss: 1.913334 12800/60000\n",
      "loss: 1.990550 19200/60000\n",
      "loss: 1.657651 25600/60000\n",
      "loss: 1.794650 32000/60000\n",
      "loss: 1.795805 38400/60000\n",
      "loss: 1.831202 44800/60000\n",
      "loss: 1.833831 51200/60000\n",
      "loss: 1.629697 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 1.799645 \n",
      "\n",
      "Epoch 6 \n",
      " ----------\n",
      "loss: 1.802362     0/60000\n",
      "loss: 1.789743  6400/60000\n",
      "loss: 1.826163 12800/60000\n",
      "loss: 1.926406 19200/60000\n",
      "loss: 1.548903 25600/60000\n",
      "loss: 1.718380 32000/60000\n",
      "loss: 1.711547 38400/60000\n",
      "loss: 1.762800 44800/60000\n",
      "loss: 1.758859 51200/60000\n",
      "loss: 1.552383 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 1.727585 \n",
      "\n",
      "Epoch 7 \n",
      " ----------\n",
      "loss: 1.725688     0/60000\n",
      "loss: 1.717472  6400/60000\n",
      "loss: 1.751318 12800/60000\n",
      "loss: 1.876573 19200/60000\n",
      "loss: 1.472371 25600/60000\n",
      "loss: 1.662801 32000/60000\n",
      "loss: 1.653247 38400/60000\n",
      "loss: 1.712392 44800/60000\n",
      "loss: 1.704763 51200/60000\n",
      "loss: 1.500154 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.674637 \n",
      "\n",
      "Epoch 8 \n",
      " ----------\n",
      "loss: 1.668191     0/60000\n",
      "loss: 1.661058  6400/60000\n",
      "loss: 1.691381 12800/60000\n",
      "loss: 1.841454 19200/60000\n",
      "loss: 1.421006 25600/60000\n",
      "loss: 1.622762 32000/60000\n",
      "loss: 1.614252 38400/60000\n",
      "loss: 1.674310 44800/60000\n",
      "loss: 1.665184 51200/60000\n",
      "loss: 1.463472 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.635488 \n",
      "\n",
      "Epoch 9 \n",
      " ----------\n",
      "loss: 1.624500     0/60000\n",
      "loss: 1.616901  6400/60000\n",
      "loss: 1.642325 12800/60000\n",
      "loss: 1.813562 19200/60000\n",
      "loss: 1.385301 25600/60000\n",
      "loss: 1.592487 32000/60000\n",
      "loss: 1.585913 38400/60000\n",
      "loss: 1.645142 44800/60000\n",
      "loss: 1.634234 51200/60000\n",
      "loss: 1.435932 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.604748 \n",
      "\n",
      "Epoch 10 \n",
      " ----------\n",
      "loss: 1.588852     0/60000\n",
      "loss: 1.580336  6400/60000\n",
      "loss: 1.601489 12800/60000\n",
      "loss: 1.791107 19200/60000\n",
      "loss: 1.359017 25600/60000\n",
      "loss: 1.568917 32000/60000\n",
      "loss: 1.563138 38400/60000\n",
      "loss: 1.620597 44800/60000\n",
      "loss: 1.591901 51200/60000\n",
      "loss: 1.372489 57600/60000\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 1.533991 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} \\n ----------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(train_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf616c7-51a9-4de7-b5f3-0c9e47ef3910",
   "metadata": {},
   "source": [
    "您可能已经注意到该模型最初不是很好（没关系！）。尝试运行循环更多的 `epochs` 或将 `learning_rate` 调整为更大的数字。也可能是这样的情况，我们选择的模型配置可能不是解决此类问题的最佳配置（事实并非如此）。后续课程将更深入地研究适用于视觉问题的模型形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b2e38-8e84-4d8f-b6e0-e984dcb38a3e",
   "metadata": {},
   "source": [
    "保存模型 \n",
    "------------- \n",
    "当您对模型的性能感到满意时，可以使用 `torch.save` 保存它。 PyTorch 模型将学习到的参数存储在internal state dictionar内部状态字典中，称为 `state_dict` 。这些可以通过 `torch.save` 方法保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e284fcd1-f0c6-43a9-9cea-dac7d4bf7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save PyToch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"data/model.pth\")\n",
    "\n",
    "print(\"Save PyToch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695daf29-b3b9-4b60-8c18-99ddc6a21c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
